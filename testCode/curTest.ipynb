{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_mar import *\n",
    "from utilsNew import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import timeit\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from confs import fn, fn2, fn22, LogFn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaid = 0\n",
    "torch.cuda.set_device(cudaid)\n",
    "#------------------------------------------------------------------------------------\n",
    "# fix the random seed for several packages\n",
    "torch.manual_seed(0) # cpu\n",
    "torch.cuda.manual_seed(0) #gpu\n",
    "np.random.seed(0) #numpy\n",
    "random.seed(0) #random and transforms\n",
    "torch.backends.cudnn.deterministic=True # cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    torch.set_default_tensor_type(torch.cuda.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conDenfs = [fn, fn2, fn22]\n",
    "f = fn\n",
    "f2 = fn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = m = 100\n",
    "p = 12\n",
    "prob = 0.2\n",
    "\n",
    "sigmaY = 1.0\n",
    "\n",
    "# generate the parameters\n",
    "beta0 = torch.cat((torch.tensor([1.0, 0, 2, 0, -3, -4, 5]), torch.zeros(p-7))) \n",
    "bTheta0 = genbTheta(n, m, sigVs=np.array([10, 9, 8, 7, 6])*10) \n",
    "initbetapref = 1 + (torch.rand(p)-1/2)/40  #[0.875, 1.125]\n",
    "initthetapref = 1 + (torch.rand(n, m)-1/2)/4\n",
    "betainit = beta0 * initbetapref\n",
    "bThetainit = bTheta0 * initthetapref\n",
    "#betainit = torch.cat((torch.tensor([0.8, 0, 1.5, 0, -2.5, -4.3, 4.8]), torch.zeros(p-7))) * 10\n",
    "#bThetainit = genbTheta(n, m, sigVs=np.array([15, 10, 7, 5, 5])/2) \n",
    "\n",
    "#X = genXUnif(n, m, p) \n",
    "X = genXBin(n, m, p, prob=prob) \n",
    "Y = genYnorm(X, bTheta0, beta0, sigmaY)\n",
    "# around 95% missing rate\n",
    "inps = {}\n",
    "inps[50] = - 8.0\n",
    "inps[100] =  -5.93\n",
    "inps[200] = -5.30\n",
    "inps[400] = -5.2\n",
    "inps[800] = -5.15\n",
    "inps[1600] = -5.1\n",
    "R = genR(Y, \"linear\", inp=inps[n], slop=1)\n",
    "R.to_dense().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = lossLpb(bTheta0, beta0, conDenfs, X, Y, R, fct=100, N=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huaqingj/MyResearch/MNAR/utilsNew.py:233: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probs = torch.tensor(probs)\n",
      "/home/huaqingj/MyResearch/MNAR/utilsNew.py:258: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probs = torch.tensor(probs)\n"
     ]
    }
   ],
   "source": [
    "v2 = lossLpbBern(bTheta0, beta0, conDenfs, X, Y, R, prob=torch.ones(p)*prob, fct=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3010e-05)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(v1 - v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
